/**
 * WebLLM Web Worker
 * åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œ WebLLM å¼•æ“ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
 */

import * as webllm from '@mlc-ai/web-llm';
import { WorkerMessage, DEFAULT_WEBLLM_MODEL, LLMMessage } from './types';

// WebGPU ç±»å‹æ‰©å±•
declare global {
    interface Navigator {
        gpu?: {
            requestAdapter(): Promise<GPUAdapter | null>;
        };
    }
    interface GPUAdapter {
        requestDevice(): Promise<GPUDevice | null>;
        requestAdapterInfo(): Promise<{ vendor: string; architecture: string }>;
    }
    interface GPUDevice { }
}

let engine: webllm.MLCEngine | null = null;

// å‘é€æ¶ˆæ¯åˆ°ä¸»çº¿ç¨‹
function postMessage(message: WorkerMessage) {
    self.postMessage(message);
}

// æ£€æµ‹ WebGPU æ”¯æŒ
async function checkWebGPU(): Promise<boolean> {
    try {
        if (!navigator.gpu) {
            console.error('âŒ WebGPU ä¸å¯ç”¨: navigator.gpu æœªå®šä¹‰');
            return false;
        }

        const adapter = await navigator.gpu.requestAdapter();
        if (!adapter) {
            console.error('âŒ WebGPU ä¸å¯ç”¨: æ— æ³•è·å– GPU é€‚é…å™¨');
            return false;
        }

        console.log('âœ… WebGPU å¯ç”¨');
        try {
            const info = await adapter.requestAdapterInfo();
            console.log('ğŸ“Š GPU é€‚é…å™¨:', info);
        } catch {
            console.log('ğŸ“Š GPU é€‚é…å™¨ä¿¡æ¯ä¸å¯ç”¨');
        }
        return true;
    } catch (error) {
        console.error('âŒ WebGPU æ£€æµ‹å¤±è´¥:', error);
        return false;
    }
}

// åˆå§‹åŒ–å¼•æ“
async function initEngine(modelId: string) {
    try {
        console.log('ğŸ”„ Worker: æ£€æµ‹ WebGPU...');

        const hasWebGPU = await checkWebGPU();
        if (!hasWebGPU) {
            postMessage({
                type: 'error',
                payload: 'WebGPU ä¸å¯ç”¨ã€‚è¯·ç¡®ä¿ä½¿ç”¨æ”¯æŒ WebGPU çš„æµè§ˆå™¨/Electron ç‰ˆæœ¬ã€‚'
            });
            return;
        }

        console.log(`ğŸ”„ Worker: å¼€å§‹åŠ è½½æ¨¡å‹ ${modelId}`);

        // å‘é€åˆå§‹è¿›åº¦ï¼ˆstage æ ‡è¯†ç¬¦ï¼ŒUI å±‚å¤„ç†ç¿»è¯‘ï¼‰
        postMessage({
            type: 'progress',
            payload: {
                stage: 'init',
                progress: 0,
                text: ''  // UI å±‚æ ¹æ® stage æ˜¾ç¤ºç¿»è¯‘æ–‡æœ¬
            }
        });

        // åˆ›å»ºå¼•æ“å¹¶è®¾ç½®è¿›åº¦å›è°ƒ
        engine = new webllm.MLCEngine({
            initProgressCallback: (progress: { text: string; progress: number }) => {
                const progressPercent = Math.round(progress.progress * 100);
                const originalText = progress.text;

                // æ ¹æ® WebLLM è¿”å›çš„æ–‡æœ¬åˆ¤æ–­æ˜¯ä¸‹è½½è¿˜æ˜¯åŠ è½½
                const isDownloading = originalText.toLowerCase().includes('fetch') ||
                    originalText.toLowerCase().includes('download');

                console.log(`ğŸ“¥ è¿›åº¦: ${progressPercent}% - ${originalText}`);
                postMessage({
                    type: 'progress',
                    payload: {
                        stage: isDownloading ? 'downloading' : 'loading',
                        progress: progressPercent,
                        text: ''  // UI å±‚æ ¹æ® stage æ˜¾ç¤ºç¿»è¯‘æ–‡æœ¬
                    }
                });
            }
        });

        await engine.reload(modelId);

        console.log('âœ… Worker: æ¨¡å‹åŠ è½½å®Œæˆ');
        postMessage({ type: 'ready' });
    } catch (error) {
        console.error('âŒ Worker: æ¨¡å‹åŠ è½½å¤±è´¥:', error);
        const errorMsg = error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯';
        postMessage({
            type: 'error',
            payload: `æ¨¡å‹åŠ è½½å¤±è´¥: ${errorMsg}`
        });
    }
}

// æµå¼èŠå¤©
async function streamChat(messages: LLMMessage[]) {
    if (!engine) {
        postMessage({
            type: 'error',
            payload: 'å¼•æ“æœªåˆå§‹åŒ–'
        });
        return;
    }

    try {
        // ä¸å†æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼ŒuseLLM çš„ buildContextPrompt å·²ç»åŒ…å«
        const fullMessages: webllm.ChatCompletionMessageParam[] = messages.map(m => ({
            role: m.role as 'user' | 'assistant' | 'system',
            content: m.content
        }));

        const asyncChunkGenerator = await engine.chat.completions.create({
            messages: fullMessages,
            stream: true,
            temperature: 0.7,
            max_tokens: 1024
        });

        for await (const chunk of asyncChunkGenerator) {
            const delta = chunk.choices[0]?.delta?.content;
            if (delta) {
                postMessage({
                    type: 'token',
                    payload: delta
                });
            }
        }

        postMessage({ type: 'complete' });
    } catch (error) {
        console.error('âŒ Worker: ç”Ÿæˆå¤±è´¥:', error);
        postMessage({
            type: 'error',
            payload: error instanceof Error ? error.message : 'ç”Ÿæˆå¤±è´¥'
        });
    }
}

// ä¸­æ­¢ç”Ÿæˆ
function abortGeneration() {
    if (engine) {
        engine.interruptGenerate();
        console.log('ğŸ›‘ Worker: ç”Ÿæˆå·²ä¸­æ­¢');
    }
}

// ç›‘å¬ä¸»çº¿ç¨‹æ¶ˆæ¯
self.onmessage = async (event: MessageEvent<WorkerMessage>) => {
    const { type, payload } = event.data;

    switch (type) {
        case 'init':
            const modelId = (payload as { modelId: string })?.modelId || DEFAULT_WEBLLM_MODEL;
            await initEngine(modelId);
            break;

        case 'chat':
            const messages = payload as LLMMessage[];
            await streamChat(messages);
            break;

        case 'abort':
            abortGeneration();
            break;

        default:
            console.warn('Worker: æœªçŸ¥æ¶ˆæ¯ç±»å‹:', type);
    }
};

console.log('ğŸ§µ WebLLM Worker å·²å¯åŠ¨');
console.log('ğŸ“Š é»˜è®¤æ¨¡å‹:', DEFAULT_WEBLLM_MODEL);
